#!/usr/bin/env python3
# coding: utf-8

__author__ = "ChenyangGao <https://chenyanggao.github.io>"
__all__: list[str] = []
__doc__ = "115 ÁΩëÁõòÊàñÂàÜ‰∫´ÈìæÊé•ÊâπÈáè‰∏ãËΩΩ"

from argparse import ArgumentParser, Namespace, RawTextHelpFormatter
from collections.abc import Mapping
from dataclasses import dataclass, field
from typing import NamedTuple, TypedDict

if __name__ == "__main__":
    from pathlib import Path
    from sys import path

    path[0] = str(Path(__file__).parents[2])
    parser = ArgumentParser(description=__doc__, formatter_class=RawTextHelpFormatter)
else:
    from .init import subparsers

    parser = subparsers.add_parser("download", description=__doc__, formatter_class=RawTextHelpFormatter)


@dataclass
class Task:
    src_attr: Mapping
    dst_path: str
    times: int = 0
    reasons: list[BaseException] = field(default_factory=list)


class Tasks(TypedDict):
    success: dict[int, Task]
    failed: dict[int, Task]
    unfinished: dict[int, Task]


class Result(NamedTuple):
    stats: dict
    tasks: Tasks


def parse_args(argv: None | list[str] = None, /) -> Namespace:
    args = parser.parse_args(argv)
    if args.version:
        from p115 import __version__
        print(".".join(map(str, __version__)))
        raise SystemExit(0)
    return args


def main(argv: None | list[str] = None, /) -> Result:
    args = parse_args(argv)

    import errno

    from collections.abc import Callable
    from contextlib import contextmanager
    from datetime import datetime
    from functools import partial
    from os import makedirs, scandir, stat
    from os.path import dirname, exists, expanduser, isdir, join as joinpath, normpath, realpath
    from platform import system
    from shutil import COPY_BUFSIZE # type: ignore
    from sys import exc_info
    from textwrap import indent
    from threading import Lock
    from traceback import format_exc
    from typing import cast, ContextManager
    from urllib.error import URLError
    from warnings import warn

    from concurrenttools import thread_batch
    from p115 import P115Client
    from rich.progress import (
        Progress, FileSizeColumn, MofNCompleteColumn, SpinnerColumn, TimeElapsedColumn, TransferSpeedColumn
    )
    from texttools import cycle_text, rotate_text
    from download import download, DEFAULT_ITER_BYTES as iter_bytes

    cookies = args.cookies
    cookies_path = args.cookies_path
    src_path = args.src_path
    dst_path = args.dst_path
    share_link = args.share_link
    lock_dir_methods = args.lock_dir_methods
    use_request = args.use_request
    max_workers = args.max_workers
    max_retries = args.max_retries
    resume = args.resume
    no_root = args.no_root

    if max_workers <= 0:
        max_workers = 1
    count_lock: None | ContextManager = None
    login_lock: None | ContextManager = None
    fs_lock: None | ContextManager = None
    if max_workers > 1:
        count_lock = Lock()
        login_lock = Lock()
        if lock_dir_methods:
            fs_lock = Lock()
    cookies_path_mtime = 0

    if not cookies:
        if cookies_path:
            try:
                cookies = open(cookies_path).read()
            except FileNotFoundError:
                pass
        else:
            seen = set()
            for dir_ in (".", expanduser("~"), dirname(__file__)):
                dir_ = realpath(dir_)
                if dir_ in seen:
                    continue
                seen.add(dir_)
                try:
                    cookies = open(joinpath(dir_, "115-cookies.txt")).read()
                    if cookies:
                        cookies_path = joinpath(dir_, "115-cookies.txt")
                        break
                except FileNotFoundError:
                    pass

    client = P115Client(cookies, app=args.app)

    urlopen: Callable
    do_request: None | Callable = None
    match use_request:
        case "httpx":
            from httpx import Client, HTTPStatusError as StatusError, RequestError
            from httpx_request import request as httpx_request
            def get_status_code(e):
                return e.response.status_code
            urlopen = partial(httpx_request, session=Client())
            iter_bytes = lambda resp: resp.iter_bytes(COPY_BUFSIZE)
        case "requests":
            try:
                from requests import Session
                from requests.exceptions import HTTPError as StatusError, RequestException as RequestError # type: ignore
                from requests_request import request as requests_request
            except ImportError:
                from sys import executable
                from subprocess import run
                run([executable, "-m", "pip", "install", "-U", "requests", "requests_request"], check=True)
                from requests import Session
                from requests.exceptions import HTTPError as StatusError, RequestException as RequestError # type: ignore
                from requests_request import request as requests_request
            do_request = urlopen = partial(requests_request, session=Session())
            iter_bytes = lambda resp: resp.iter_content(COPY_BUFSIZE)
            def get_status_code(e):
                return e.response.status_code
        case "urllib3":
            from urllib.error import HTTPError as StatusError # type: ignore
            try:
                from urllib3.exceptions import RequestError # type: ignore
                from urllib3_request import request as urllib3_request
            except ImportError:
                from sys import executable
                from subprocess import run
                run([executable, "-m", "pip", "install", "-U", "urllib3", "urllib3_request"], check=True)
                from urllib3.exceptions import RequestError # type: ignore
                from urllib3_request import request as urllib3_request
            do_request = urlopen = urllib3_request
            def get_status_code(e):
                return e.status
        case "urlopen":
            from urllib.error import HTTPError as StatusError, URLError as RequestError # type: ignore
            from urllib.request import build_opener, HTTPCookieProcessor
            try:
                from urlopen import request as urlopen_request
            except ImportError:
                from sys import executable
                from subprocess import run
                run([executable, "-m", "pip", "install", "-U", "python-urlopen"], check=True)
                from urlopen import request as urlopen_request
            do_request = urlopen = partial(urlopen_request, opener=build_opener(HTTPCookieProcessor(client.cookiejar)))
            def get_status_code(e):
                return e.status

    device = client.login_device(request=do_request)["icon"]
    if device not in AVAILABLE_APPS:
        # 115 ÊµèËßàÂô®Áâà
        if device == "desktop":
            device = "web"
        else:
            warn(f"encountered an unsupported app {device!r}, fall back to 'qandroid'")
            device = "qandroid"
    if cookies_path and cookies != client.cookies:
        open(cookies_path, "w").write(client.cookies)

    if share_link:
        fs = client.get_share_fs(share_link, request=do_request)
    else:
        fs = client.get_fs(request=do_request)

    match system():
        case "Windows":
            transtab = str.maketrans('<>/\\|:*?"', 'ÔºúÔºûÔºèÔººÔΩúÔºöÔºäÔºüÔºÇ')
            def escape_name(name):
                return name.translate(transtab)
        case "Darwin":
            transtab = {ord("/"): ord(":"), ord(":"): ord("Ôºö")}
            def escape_name(name):
                return name.translate(transtab)
        case "Linux":
            def escape_name(name):
                return name.replace("/", "Ôºè")

    @contextmanager
    def ensure_cm(cm):
        if isinstance(cm, ContextManager):
            with cm as val:
                yield val
        else:
            yield cm

    def relogin(exc=None):
        nonlocal cookies_path_mtime
        if exc is None:
            exc = exc_info()[0]
        mtime = cookies_path_mtime
        with ensure_cm(login_lock):
            need_update = mtime == cookies_path_mtime
            if cookies_path and need_update:
                try:
                    mtime = stat(cookies_path).st_mtime_ns
                    if mtime != cookies_path_mtime:
                        client.cookies = open(cookies_path).read()
                        cookies_path_mtime = mtime
                        need_update = False
                except FileNotFoundError:
                    console_print("[bold yellow][SCAN] ü¶æ Êñá‰ª∂Á©∫Áº∫[/bold yellow]")
            if need_update:
                if exc is None:
                    console_print("[bold yellow][SCAN] ü¶æ ÈáçÊñ∞Êâ´Á†Å[/bold yellow]")
                else:
                    console_print("""{prompt}‰∏Ä‰∏™ Web API ÂèóÈôê (ÂìçÂ∫î "405: Not Allowed"), Â∞ÜËá™Âä®Êâ´Á†ÅÁôªÂΩïÂêå‰∏ÄËÆæÂ§á\n{exc}""".format(
                        prompt = "[bold yellow][SCAN] ü§ñ ÈáçÊñ∞Êâ´Á†ÅÔºö[/bold yellow]", 
                        exc    = f"    ‚îú [red]{type(exc).__qualname__}[/red]: {exc}")
                    )
                client.login_another_app(device, request=do_request, replace=True, timeout=5)
                if cookies_path:
                    open(cookies_path, "w").write(client.cookies)
                    cookies_path_mtime = stat(cookies_path).st_mtime_ns

    def relogin_wrap(func, /, *args, **kwds):
        try:
            with ensure_cm(fs_lock):
                return func(*args, **kwds)
        except StatusError as e:
            if get_status_code(e) != 405:
                raise
            relogin(e)
        return relogin_wrap(func, *args, **kwds)

    stats: dict = {
        # ÂºÄÂßãÊó∂Èó¥
        "start_time": datetime.now(), 
        # ÊÄªËÄóÊó∂
        "elapsed": "", 
        # Ê∫êË∑ØÂæÑ
        "src_path": "", 
        # ÁõÆÊ†áË∑ØÂæÑ
        "dst_path": "", 
        # ‰ªªÂä°ÊÄªÊï∞
        "tasks": {"total": 0, "files": 0, "dirs": 0, "size": 0}, 
        # ÊàêÂäü‰ªªÂä°Êï∞
        "success": {"total": 0, "files": 0, "dirs": 0, "size": 0}, 
        # Â§±Ë¥•‰ªªÂä°Êï∞ÔºàÂèëÁîüÈîôËØØ‰ΩÜÂ∑≤ÊäõÂºÉÔºâ
        "failed": {"total": 0, "files": 0, "dirs": 0, "size": 0}, 
        # ÈáçËØï‰ªªÂä°Êï∞ÔºàÂèëÁîüÈîôËØØ‰ΩÜÂèØÈáçËØïÔºâÔºå‰∏Ä‰∏™‰ªªÂä°ÂèØ‰ª•ÈáçËØïÂ§öÊ¨°
        "retry": {"total": 0, "files": 0, "dirs": 0}, 
        # Êú™ÂÆåÊàê‰ªªÂä°Êï∞ÔºöÊú™ËøêË°å„ÄÅÈáçËØï‰∏≠ÊàñËøêË°å‰∏≠
        "unfinished": {"total": 0, "files": 0, "dirs": 0, "size": 0}, 
        # ÂêÑÁßçÈîôËØØÊï∞ÈáèÂíåÂàÜÁ±ªÊ±áÊÄª
        "errors": {"total": 0, "files": 0, "dirs": 0, "reasons": {}}, 
        # ÊòØÂê¶ÊâßË°åÂÆåÊàêÔºöÂ¶ÇÊûúÊòØ FalseÔºåËØ¥ÊòéÊòØË¢´‰∫∫‰∏∫ÁªàÊ≠¢
        "is_completed": False, 
    }
    # ‰ªªÂä°ÊÄªÊï∞
    tasks: dict[str, int] = stats["tasks"]
    # ÊàêÂäü‰ªªÂä°Êï∞
    success: dict[str, int] = stats["success"]
    # Â§±Ë¥•‰ªªÂä°Êï∞ÔºàÂèëÁîüÈîôËØØ‰ΩÜÂ∑≤ÊäõÂºÉÔºâ
    failed: dict[str, int] = stats["failed"]
    # ÈáçËØï‰ªªÂä°Êï∞ÔºàÂèëÁîüÈîôËØØ‰ΩÜÂèØÈáçËØïÔºâÔºå‰∏Ä‰∏™‰ªªÂä°ÂèØ‰ª•ÈáçËØïÂ§öÊ¨°
    retry: dict[str, int] = stats["retry"]
    # Êú™ÂÆåÊàê‰ªªÂä°Êï∞ÔºöÊú™ËøêË°å„ÄÅÈáçËØï‰∏≠ÊàñËøêË°å‰∏≠
    unfinished: dict[str, int] = stats["unfinished"]
    # ÂêÑÁßçÈîôËØØÊï∞ÈáèÂíåÂàÜÁ±ªÊ±áÊÄª
    errors: dict = stats["errors"]
    # ÂêÑÁßçÈîôËØØÁöÑÂàÜÁ±ªÊ±áÊÄª
    reasons: dict[str, int] = errors["reasons"]
    # ÂºÄÂßãÊó∂Èó¥
    start_time = stats["start_time"]

    def update_tasks(total=1, files=0, size=0):
        dirs = total - files
        with ensure_cm(count_lock):
            tasks["total"] += total
            unfinished["total"] += total
            if dirs:
                tasks["dirs"] += dirs
                unfinished["dirs"] += dirs
            if files:
                tasks["files"] += files
                tasks["size"] += size
                unfinished["files"] += files
                unfinished["size"] += size

    def update_success(total=1, files=0, size=0):
        dirs = total - files
        with ensure_cm(count_lock):
            success["total"] += total
            unfinished["total"] -= total
            if dirs:
                success["dirs"] += dirs
                unfinished["dirs"] -= dirs
            if files:
                success["files"] += files
                success["size"] += size
                unfinished["files"] -= files
                unfinished["size"] -= size

    def update_failed(total=1, files=0, size=0):
        dirs = total - files
        with ensure_cm(count_lock):
            failed["total"] += total
            unfinished["total"] -= total
            if dirs:
                failed["dirs"] += dirs
                unfinished["dirs"] -= dirs
            if files:
                failed["files"] += files
                failed["size"] += size
                unfinished["files"] -= files
                unfinished["size"] -= size

    def update_retry(total=1, files=0):
        dirs = total - files
        with ensure_cm(count_lock):
            retry["total"] += total
            if dirs:
                retry["dirs"] += dirs
            if files:
                retry["files"] += files

    def update_errors(e, is_directory=False):
        exctype = type(e).__module__ + "." + type(e).__qualname__
        with ensure_cm(count_lock):
            errors["total"] += 1
            if is_directory:
                errors["dirs"] += 1
            else:
                errors["files"] += 1
            try:
                reasons[exctype] += 1
            except KeyError:
                reasons[exctype] = 1

    def add_report(_, attr):
        update_desc = rotate_text(attr["name"], 32, interval=0.1).__next__
        task = progress.add_task(update_desc(), total=attr["size"])
        try:
            while not closed:
                progress.update(task, description=update_desc(), advance=(yield))
        finally:
            progress.remove_task(task)

    def get_url(attr) -> str:
        if share_link:
            return fs.get_url(attr["id"])
        if attr.get("violated", False):
            if attr["size"] >= 1024 * 1024 * 115:
                return ""
            return fs.get_url_from_pickcode(attr["pickcode"], use_web_api=True)
        else:
            return fs.get_url_from_pickcode(attr["pickcode"])

    def work(task: Task, submit):
        attr, dst_path = task.src_attr, task.dst_path
        task_id = attr["id"]
        try:
            task.times += 1
            if attr["is_directory"]:
                try:
                    sub_entries = {entry.name: entry for entry in scandir(dst_path)}
                except FileNotFoundError:
                    makedirs(dst_path, exist_ok=True)
                    sub_entries = {}
                    console_print(f"[bold green][GOOD][/bold green] üìÇ ÂàõÂª∫ÁõÆÂΩï: [blue underline]{attr['path']!r}[/blue underline] ‚ûú [blue underline]{dst_path!r}[/blue underline]")

                subattrs = relogin_wrap(fs.listdir_attr, task_id)
                update_tasks(
                    total=len(subattrs), 
                    files=sum(not a["is_directory"] for a in subattrs), 
                    size=sum(a["size"] for a in subattrs if not a["is_directory"]), 
                )
                progress.update(statistics_bar, total=tasks["total"], description=update_stats_desc())
                seen: set[str] = set()
                for subattr in subattrs:
                    subpath = subattr["path"]
                    name = escape_name(subattr["name"])
                    subdpath = joinpath(dst_path, name)
                    if name in seen:
                        console_print(f"[bold red][FAIL][/bold red] üóëÔ∏è ÂêçÁß∞ÂÜ≤Á™ÅÔºàÂ∞ÜÊäõÂºÉÔºâ: [blue underline]{subpath!r}[/blue underline] ‚ûú [blue underline]{subdpath!r}[/blue underline]")
                        continue
                    if name in sub_entries:
                        entry = sub_entries[name]
                        is_directory = subattr["is_directory"]
                        if is_directory != entry.is_dir(follow_symlinks=True):
                            console_print(f"[bold red][FAIL][/bold red] üí© Á±ªÂûãÂ§±ÈÖçÔºàÂ∞ÜÊäõÂºÉÔºâ: [blue underline]{subpath!r}[/blue underline] ‚ûú [blue underline]{subdpath!r}[/blue underline]")
                            update_failed(1, not is_directory, subattr.get("size"))
                            progress.update(statistics_bar, advance=1, description=update_stats_desc())
                            continue
                        elif is_directory:
                            console_print(f"[bold yellow][SKIP][/bold yellow] üìÇ ÁõÆÂΩïÂ∑≤Âª∫: [blue underline]{subpath!r}[/blue underline] ‚ûú [blue underline]{subdpath!r}[/blue underline]")
                        elif resume and not is_directory and subattr["size"] == entry.stat().st_size:
                            console_print(f"[bold yellow][SKIP][/bold yellow] üìù Ë∑≥ËøáÊñá‰ª∂: [blue underline]{subpath!r}[/blue underline] ‚ûú [blue underline]{subdpath!r}[/blue underline]")
                            update_success(1, 1, subattr["size"])
                            progress.update(statistics_bar, advance=1, description=update_stats_desc())
                            continue
                    seen.add(name)
                    subtask = unfinished_tasks[subattr["id"]] = Task(subattr, joinpath(dst_path, name))
                    submit(subtask)
                update_success(1)
            else:
                url = get_url(attr)
                if not url:
                    raise OSError(errno.ENODATA, f"can't get url for {attr!r}")
                download(
                    url, 
                    dst_path, 
                    resume=resume, 
                    make_reporthook=partial(add_report, attr=attr), 
                    urlopen=urlopen, 
                    iter_bytes=iter_bytes, 
                )
                console_print(f"[bold green][GOOD][/bold green] üìù ‰∏ãËΩΩÊñá‰ª∂: [blue underline]{attr['path']!r}[/blue underline] ‚ûú [blue underline]{dst_path!r}[/blue underline]")
                update_success(1, 1, attr["size"])
            progress.update(statistics_bar, advance=1, description=update_stats_desc())
            success_tasks[task_id] = unfinished_tasks.pop(task_id)
        except BaseException as e:
            task.reasons.append(e)
            update_errors(e, attr["is_directory"])
            if max_retries < 0:
                if isinstance(e, StatusError):
                    status_code = get_status_code(e)
                    if status_code == 405:
                        retryable = True
                        try:
                            relogin()
                        except:
                            pass
                    else:
                        retryable = not (400 <= status_code < 500)
                else:
                    retryable = isinstance(e, (RequestError, URLError, TimeoutError))
            else:
                retryable = task.times <= max_retries
            if retryable:
                console_print(f"""\
[bold red][FAIL][/bold red] ‚ôªÔ∏è ÂèëÁîüÈîôËØØÔºàÂ∞ÜÈáçËØïÔºâ: [blue underline]{attr['path']!r}[/blue underline] ‚ûú [blue underline]{dst_path!r}[/blue underline]
    ‚îú {type(e).__qualname__}: {e}""")
                update_retry(1, not attr["is_directory"])
                submit(task)
            else:
                console_print(f"""\
[bold red][FAIL][/bold red] üíÄ ÂèëÁîüÈîôËØØÔºàÂ∞ÜÊäõÂºÉÔºâ: [blue underline]{attr['path']!r}[/blue underline] ‚ûú [blue underline]{dst_path!r}[/blue underline]
{indent(format_exc().strip(), "    ‚îú ")}""")
                progress.update(statistics_bar, advance=1, description=update_stats_desc())
                update_failed(1, not attr["is_directory"], attr.get("size"))
                failed_tasks[task_id] = unfinished_tasks.pop(task_id)
                if len(task.reasons) == 1:
                    raise
                else:
                    raise BaseExceptionGroup('max retries exceed', task.reasons)

    with Progress(
        SpinnerColumn(), 
        *Progress.get_default_columns(), 
        TimeElapsedColumn(), 
        MofNCompleteColumn(), 
        TransferSpeedColumn(), 
        FileSizeColumn(), 
    ) as progress:
        console_print = progress.console.print
        if isinstance(src_path, str):
            if src_path == "0":
                src_path = "/"
            elif not src_path.startswith("0") and src_path.isascii() and src_path.isdecimal():
                src_path = int(src_path)
        src_attr = relogin_wrap(fs.attr, src_path)
        is_directory = src_attr["is_directory"]
        name = escape_name(src_attr["name"])
        dst_path = normpath(dst_path)
        if exists(dst_path):
            dst_path_isdir = isdir(dst_path)
            if is_directory:
                if not dst_path_isdir:
                    raise NotADirectoryError(errno.ENOTDIR, f"{dst_path!r} is not directory")
                elif name and not no_root:
                    dst_path = joinpath(dst_path, name)
                    makedirs(dst_path, exist_ok=True)
            elif name and dst_path_isdir:
                dst_path = joinpath(dst_path, name)
                if isdir(dst_path):
                    raise IsADirectoryError(errno.EISDIR, f"{dst_path!r} is directory")
        elif is_directory:
            if no_root or not name:
                makedirs(dst_path)
            else:
                dst_path = joinpath(dst_path, name)
                makedirs(dst_path)
        unfinished_tasks: dict[int, Task] = {src_attr["id"]: Task(src_attr, dst_path)}
        success_tasks: dict[int, Task] = {}
        failed_tasks: dict[int, Task] = {}
        all_tasks: Tasks = {
            "success": success_tasks, 
            "failed": failed_tasks, 
            "unfinished": unfinished_tasks, 
        }
        stats["src_path"] = src_attr["path"]
        stats["dst_path"] = dst_path
        update_tasks(1, not src_attr["is_directory"], src_attr.get("size"))
        update_stats_desc = cycle_text(
            ("...", "..", ".", ".."), 
            prefix="üìä [cyan bold]statistics[/cyan bold] ", 
            min_length=32 + 23, 
            interval=0.1, 
        ).__next__
        statistics_bar = progress.add_task(update_stats_desc(), total=1)
        closed = False
        try:
            thread_batch(work, unfinished_tasks.values(), max_workers=max_workers)
            stats["is_completed"] = True
        finally:
            closed = True
            progress.remove_task(statistics_bar)
            stats["elapsed"] = str(datetime.now() - start_time)
            console_print(f"üìä [cyan bold]statistics:[/cyan bold] {stats}")
    return Result(stats, all_tasks)


from p115 import AVAILABLE_APPS

parser.add_argument("-c", "--cookies", help="115 ÁôªÂΩï cookiesÔºå‰ºòÂÖàÁ∫ßÈ´ò‰∫é -cp/--cookies-path")
parser.add_argument("-cp", "--cookies-path", help="""\
Â≠òÂÇ® 115 ÁôªÂΩï cookies ÁöÑÊñáÊú¨Êñá‰ª∂ÁöÑË∑ØÂæÑÔºåÂ¶ÇÊûúÁº∫Â§±ÔºåÂàô‰ªé 115-cookies.txt Êñá‰ª∂‰∏≠Ëé∑ÂèñÔºåÊ≠§Êñá‰ª∂ÂèØÂú®Â¶Ç‰∏ãÁõÆÂΩï‰πã‰∏Ä: 
    1. ÂΩìÂâçÂ∑•‰ΩúÁõÆÂΩï
    2. Áî®Êà∑Ê†πÁõÆÂΩï
    3. Ê≠§ËÑöÊú¨ÊâÄÂú®ÁõÆÂΩï""")
parser.add_argument(
    "-a", "--app", default="qandroid", 
    choices=AVAILABLE_APPS, 
    help="ÂøÖË¶ÅÊó∂ÔºåÈÄâÊã©‰∏Ä‰∏™ app ËøõË°åÊâ´Á†ÅÁôªÂΩïÔºåÈªòËÆ§ÂÄº 'qandroid'ÔºåÊ≥®ÊÑèÔºöËøô‰ºöÊääÂ∑≤ÁªèÁôªÂΩïÁöÑÁõ∏Âêå app Ë∏¢‰∏ãÁ∫ø")
parser.add_argument("-p", "--src-path", default="/", help="115 ÁΩëÁõò‰∏≠ÁöÑÊñá‰ª∂ÊàñÁõÆÂΩïÁöÑ id ÊàñË∑ØÂæÑÔºåÈªòËÆ§ÂÄºÔºö'/")
parser.add_argument("-t", "--dst-path", default=".", help="Êú¨Âú∞ÁöÑË∑ØÂæÑÔºåÈªòËÆ§ÊòØÂΩìÂâçÂ∑•‰ΩúÁõÆÂΩïÔºåÂç≥ '.'")
parser.add_argument("-s", "--share-link", nargs="?", help="""\
115 ÁöÑÂàÜ‰∫´ÈìæÊé•
    1. ÊåáÂÆö‰∫ÜÂàô‰ªéÂàÜ‰∫´ÈìæÊé•‰∏ãËΩΩ
    2. ‰∏çÊåáÂÆöÂàô‰ªé 115 ÁΩëÁõò‰∏ãËΩΩ""")
parser.add_argument("-m", "--max-workers", default=1, type=int, help="Âπ∂ÂèëÁ∫øÁ®ãÊï∞ÔºåÈªòËÆ§ÂÄº 1")
parser.add_argument("-mr", "--max-retries", default=-1, type=int, 
                    help="""ÊúÄÂ§ßÈáçËØïÊ¨°Êï∞„ÄÇ
    - Â¶ÇÊûúÂ∞è‰∫é 0ÔºàÈªòËÆ§ÔºâÔºåÂàô‰ºöÂØπ‰∏Ä‰∫õË∂ÖÊó∂„ÄÅÁΩëÁªúËØ∑Ê±ÇÈîôËØØËøõË°åÊó†ÈôêÈáçËØïÔºåÂÖ∂ÂÆÉÈîôËØØËøõË°åÊäõÂá∫
    - Â¶ÇÊûúÁ≠â‰∫é 0ÔºåÂàôÂèëÁîüÈîôËØØÂ∞±ÊäõÂá∫
    - Â¶ÇÊûúÂ§ß‰∫é 0ÔºàÂÆûÈôÖÊâßË°å 1+n Ê¨°ÔºåÁ¨¨‰∏ÄÊ¨°‰∏çÂè´ÈáçËØïÔºâÔºåÂàôÂØπÊâÄÊúâÈîôËØØÁ≠âÁ±ªÈΩêËßÇÔºåÂè™Ë¶ÅÊ¨°Êï∞Âà∞ËææÊ≠§Êï∞ÂÄºÂ∞±ÊäõÂá∫""")
parser.add_argument("-l", "--lock-dir-methods", action="store_true", 
                    help="ÂØπ 115 ÁöÑÊñá‰ª∂Á≥ªÁªüËøõË°åÂ¢ûÂà†ÊîπÊü•ÁöÑÊìç‰ΩúÔºà‰ΩÜ‰∏çÂåÖÊã¨‰∏ä‰º†Âíå‰∏ãËΩΩÔºâËøõË°åÂä†ÈîÅÔºåÈôêÂà∂‰∏∫ÂçïÁ∫øÁ®ãÔºåËøôÊ†∑Â∞±ÂèØÂáèÂ∞ë 405 ÂìçÂ∫îÔºå‰ª•Èôç‰ΩéÊâ´Á†ÅÁöÑÈ¢ëÁéá")
parser.add_argument("-ur", "--use-request", choices=("httpx", "requests", "urllib3", "urlopen"), default="httpx", help="ÈÄâÊã©‰∏Ä‰∏™ÁΩëÁªúËØ∑Ê±ÇÊ®°ÂùóÔºåÈªòËÆ§ÂÄºÔºöhttpx")
parser.add_argument("-n", "--no-root", action="store_true", help="‰∏ãËΩΩÁõÆÂΩïÊó∂ÔºåÁõ¥Êé•ÂêàÂπ∂Âà∞ÁõÆÊ†áÁõÆÂΩïÔºåËÄå‰∏çÊòØÂà∞‰∏éÊ∫êÁõÆÂΩïÂêåÂêçÁöÑÂ≠êÁõÆÂΩï")
parser.add_argument("-r", "--resume", action="store_true", help="Êñ≠ÁÇπÁª≠‰º†")
parser.add_argument("-v", "--version", action="store_true", help="ËæìÂá∫ÁâàÊú¨Âè∑")
parser.set_defaults(func=main)


if __name__ == "__main__":
    main()

