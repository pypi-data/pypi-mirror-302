<div align="center">

<img alt="VisActor Logo" width=50% src="https://github.com/DoodleBears/split-lang/blob/main/.github/profile/split-lang-logo.svg"/>

<img alt="VisActor Logo" width=70% src="https://github.com/DoodleBears/split-lang/blob/main/.github/profile/split-lang-banner.svg"/>
  
</div>
<div align="center">
  <h1>split-lang</h1>

**English** | [**‰∏≠ÊñáÁÆÄ‰Ωì**](./docs/zh/README.md) | [**Êó•Êú¨Ë™û**](./docs/ja/README.md)

Split text by languages through concatenating over split substrings based on their language, powered by

splitting: [`budoux`](https://github.com/google/budoux) and rule-base splitting

language detection: [`fast-langdetect`](https://github.com/LlmKira/fast-langdetect) and [`wordfreq`](https://github.com/rspeer/wordfreq)

</div>

<br/>

<div align="center">

[![PyPI version](https://badge.fury.io/py/split-lang.svg)](https://badge.fury.io/py/split-lang)
[![Downloads](https://static.pepy.tech/badge/split-lang)](https://pepy.tech/project/split-lang)
[![Downloads](https://static.pepy.tech/badge/split-lang/month)](https://pepy.tech/project/split-lang)


[![Open In Colab](https://colab.research.google.com/assets/colab-badge.svg)](https://colab.research.google.com/github/DoodleBears/split-lang/blob/main/split-lang-demo.ipynb)


[![License](https://img.shields.io/badge/license-MIT-green.svg)](https://github.com/DoodleBears/split-lang/blob/main/LICENSE)
![GitHub Repo stars](https://img.shields.io/github/stars/DoodleBears/split-lang)
[![wakatime](https://wakatime.com/badge/user/5728d95a-5cfb-4acb-b600-e34c2fc231b6/project/e06e0a00-9ba1-453d-8c62-a0b2604aaaad.svg)](https://wakatime.com/badge/user/5728d95a-5cfb-4acb-b600-e34c2fc231b6/project/e06e0a00-9ba1-453d-8c62-a0b2604aaaad)

</div>




# 1. üí°How it works

**Stage 1**: rule-based split (separate character, punctuation and digit)
- `hello, how are you` -> `hello` | `,` | `how are you`

**Stage 2**: over-split text to substrings by [`budoux`](https://github.com/google/budoux) for Chinese mix with Japanese, ` ` (space) for **not** [scripta continua](https://en.wikipedia.org/wiki/Scriptio_continua)
- `‰Ω†ÂñúÊ¨¢Áúã„Ç¢„Éã„É°Âêó` -> `‰Ω†` | `ÂñúÊ¨¢` | `Áúã` | `„Ç¢„Éã„É°` | `Âêó`
- `Êò®Â§©Ë¶ã„ÅüÊò†Áîª„ÅØ„Å®„Å¶„ÇÇÊÑüÂãïÁöÑ„Åß„Åó„Åü` -> `Êò®Â§©` | `Ë¶ã„Åü` | `Êò†Áîª` | `„ÅØ` | `„Å®„Å¶„ÇÇ` | `ÊÑüÂãï` | `ÁöÑ` | `„Åß` | `„Åó„Åü`
- `how are you` -> `how ` | `are ` | `you`

**Stage 3**: concatenate substrings based on their languages using [`fast-langdetect`](https://github.com/LlmKira/fast-langdetect), [`wordfreq`](https://github.com/rspeer/wordfreq) and regex (rule-based)
- `‰Ω†` | `ÂñúÊ¨¢` | `Áúã` | `„Ç¢„Éã„É°` | `Âêó` -> `‰Ω†ÂñúÊ¨¢Áúã` | `„Ç¢„Éã„É°` | `Âêó`
- `Êò®Â§©` | `Ë¶ã„Åü` | `Êò†Áîª` | `„ÅØ` | `„Å®„Å¶„ÇÇ` | `ÊÑüÂãï` | `ÁöÑ` | `„Åß` | `„Åó„Åü` -> `Êò®Â§©` | `Ë¶ã„ÅüÊò†Áîª„ÅØ„Å®„Å¶„ÇÇÊÑüÂãïÁöÑ„Åß„Åó„Åü`
- `how ` | `are ` | `you` -> `how are you`

# 2. ü™®Motivation
- `TTS (Text-To-Speech)` model often **fails** on multi-language speech generation, there are two ways to do:
  - Train a model can pronounce multiple languages
  - **(This Package)** Separate sentence based on language first, then use different language models
- Existed models in NLP toolkit (e.g. `SpaCy`, `jieba`) is usually helpful for dealing with text in **ONE** language for each model. Which means multi-language texts need pre-process, like texts below: 

```
‰Ω†ÂñúÊ¨¢Áúã„Ç¢„Éã„É°ÂêóÔºü
Vielen Dank merci beaucoup for your help.
‰Ω†ÊúÄËøëÂ•ΩÂêó„ÄÅÊúÄËøë„Å©„ÅÜ„Åß„Åô„ÅãÔºüÏöîÏ¶ò Ïñ¥ÎñªÍ≤å ÏßÄÎÇ¥ÏöîÔºüsky is clear and sunny„ÄÇ
```

- [1. üí°How it works](#1-how-it-works)
- [2. ü™®Motivation](#2-motivation)
- [3. üìïUsage](#3-usage)
  - [3.1. üöÄInstallation](#31-installation)
  - [3.2. Basic](#32-basic)
    - [3.2.1. `split_by_lang`](#321-split_by_lang)
    - [3.2.2. `merge_across_digit`](#322-merge_across_digit)
  - [3.3. Advanced](#33-advanced)
    - [3.3.1. usage of `lang_map` and `default_lang` (for your languages)](#331-usage-of-lang_map-and-default_lang-for-your-languages)
- [4. Acknowledgement](#4-acknowledgement)
- [5. ‚ú®Star History](#5-star-history)


# 3. üìïUsage

## 3.1. üöÄInstallation

You can install the package using pip:

```bash
pip install split-lang
```


****
## 3.2. Basic
### 3.2.1. `split_by_lang`

[![Open In Colab](https://colab.research.google.com/assets/colab-badge.svg)](https://colab.research.google.com/github/DoodleBears/split-lang/blob/main/split-lang-demo.ipynb)

```python
from split_lang import LangSplitter
lang_splitter = LangSplitter()
text = "‰Ω†ÂñúÊ¨¢Áúã„Ç¢„Éã„É°Âêó"

substr = lang_splitter.split_by_lang(
    text=text,
)
for index, item in enumerate(substr):
    print(f"{index}|{item.lang}:{item.text}")
```

```
0|zh:‰Ω†ÂñúÊ¨¢Áúã
1|ja:„Ç¢„Éã„É°
2|zh:Âêó
```

```python
from split_lang import LangSplitter
lang_splitter = LangSplitter(merge_across_punctuation=True)
import time
texts = [
    "‰Ω†ÂñúÊ¨¢Áúã„Ç¢„Éã„É°ÂêóÔºüÊàë‰πüÂñúÊ¨¢Áúã",
    "Please star this project on GitHub, Thanks you. I love youËØ∑Âä†ÊòüËøô‰∏™È°πÁõÆÔºåË∞¢Ë∞¢‰Ω†„ÄÇÊàëÁà±‰Ω†„Åì„ÅÆÈ†ÖÁõÆ„Çí„Çπ„Çø„Éº„Åó„Å¶„Åè„Å†„Åï„ÅÑ„ÄÅ„ÅÇ„Çä„Åå„Å®„ÅÜ„Åî„Åñ„ÅÑ„Åæ„ÅôÔºÅÊÑõ„Åó„Å¶„Çã",
]
time1 = time.time()
for text in texts:
    substr = lang_splitter.split_by_lang(
        text=text,
    )
    for index, item in enumerate(substr):
        print(f"{index}|{item.lang}:{item.text}")
    print("----------------------")
time2 = time.time()
print(time2 - time1)
```

```
0|zh:‰Ω†ÂñúÊ¨¢Áúã
1|ja:„Ç¢„Éã„É°
2|zh:ÂêóÔºüÊàë‰πüÂñúÊ¨¢Áúã
----------------------
0|en:Please star this project on GitHub, Thanks you. I love you
1|zh:ËØ∑Âä†ÊòüËøô‰∏™È°πÁõÆÔºåË∞¢Ë∞¢‰Ω†„ÄÇÊàëÁà±‰Ω†
2|ja:„Åì„ÅÆÈ†ÖÁõÆ„Çí„Çπ„Çø„Éº„Åó„Å¶„Åè„Å†„Åï„ÅÑ„ÄÅ„ÅÇ„Çä„Åå„Å®„ÅÜ„Åî„Åñ„ÅÑ„Åæ„ÅôÔºÅÊÑõ„Åó„Å¶„Çã
----------------------
0.007998466491699219
```

### 3.2.2. `merge_across_digit`

```python
lang_splitter.merge_across_digit = False
texts = [
    "Ë°¨Ë°´ÁöÑ‰ª∑Ê†ºÊòØ9.15‰æøÂ£´",
]
for text in texts:
    substr = lang_splitter.split_by_lang(
        text=text,
    )
    for index, item in enumerate(substr):
        print(f"{index}|{item.lang}:{item.text}")
```

```
0|zh:Ë°¨Ë°´ÁöÑ‰ª∑Ê†ºÊòØ
1|digit:9.15
2|zh:‰æøÂ£´
```

## 3.3. Advanced

### 3.3.1. usage of `lang_map` and `default_lang` (for your languages)

> [!IMPORTANT]
> Add lang code for your usecase if other languages are needed. [See Support Language](https://github.com/zafercavdar/fasttext-langdetect#supported-languages)

- default `lang_map` looks like below
  - if `langua-py` or `fasttext` or any other language detector detect the language that is NOT included in `lang_map` will be set to `default_lang`
  - if you set `default_lang` or `value` of `key:value` in `lang_map` to `x`, this substring will be merged to the near substring
    - `zh` | `x` | `jp` -> `zh` | `jp` (`x` been merged to one side)
    - In example below, `zh-tw` is set to `x` because character in `zh` and `jp` sometimes been detected as Traditional Chinese
- default `default_lang` is `x`

```python
DEFAULT_LANG_MAP = {
    "zh": "zh",
    "yue": "zh",  # Á≤§ËØ≠
    "wuu": "zh",  # Âê¥ËØ≠
    "zh-cn": "zh",
    "zh-tw": "x",
    "ko": "ko",
    "ja": "ja",
    "de": "de",
    "fr": "fr",
    "en": "en",
    "hr": "en",
}
DEFAULT_LANG = "x"

```

# 4. Acknowledgement

- Inspired by [LlmKira/fast-langdetect](https://github.com/LlmKira/fast-langdetect)
- Text segmentation depends on [google/budoux](https://github.com/google/budoux)
- Language detection depends on [zafercavdar/fasttext-langdetect](https://github.com/zafercavdar/fasttext-langdetect) and [rspeer/wordfreq](https://github.com/rspeer/wordfreq)

# 5. ‚ú®Star History

[![Star History Chart](https://api.star-history.com/svg?repos=DoodleBears/split-lang&type=Timeline)](https://star-history.com/#DoodleBears/split-lang&Timeline)