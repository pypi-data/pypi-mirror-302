{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Backtest MBRL-PETS strategy on Single Instrument Environment"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import hydra\n",
    "import numpy as np\n",
    "from omegaconf import DictConfig, OmegaConf\n",
    "from PIL import Image\n",
    "from pathlib import Path\n",
    "import matplotlib.pyplot as plt\n",
    "import gymnasium as gym\n",
    "\n",
    "import deeptrade.models as models\n",
    "import deeptrade.planning as planning\n",
    "import deeptrade.util\n",
    "import deeptrade.util.common as common_utils\n",
    "import deeptrade.util.replay_buffer as replay_buffer\n",
    "\n",
    "import deeptrade.env\n",
    "import deeptrade.env.termination_fns as term_fns\n",
    "import deeptrade.env.reward_fns as reward_fns\n",
    "\n",
    "import deeptrade.env.cartpole_continuous as cartpole_env"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "seed = 0\n",
    "ensemble_size = 5\n",
    "num_trials = 50\n",
    "trial_length = 200\n",
    "device = 'cuda:0' if torch.cuda.is_available() else 'cpu'\n",
    "\n",
    "# Setup Environment\n",
    "env = gym.make(\"SingleInstrument-v0\", seed=seed, price_gen_info={\"starting_price\": 1000.0, \"mean\": 2.0, \"std\": 0.1, \"n_days\": 100})\n",
    "# env = cartpole_env.CartPoleEnv(render_mode=\"rgb_array\")\n",
    "env.reset(seed=seed)\n",
    "rng = np.random.default_rng(seed=seed)\n",
    "generator = torch.Generator(device=device)\n",
    "generator.manual_seed(seed)\n",
    "obs_shape = env.observation_space.shape\n",
    "act_shape = env.action_space.shape\n",
    "reward_fn = reward_fns.single_instrument\n",
    "term_fn = term_fns.margin_call\n",
    "\n",
    "# reward_fn = reward_fns.cartpole\n",
    "# term_fn = term_fns.cartpole\n",
    "\n",
    "# in_size = obs_shape[0] + act_shape[0]\n",
    "# out_size = obs_shape[0]\n",
    "\n",
    "cfg_dict = {\n",
    "    \"dynamics_model\": {\n",
    "        \"_target_\": \"deeptrade.models.GaussianMLP\",\n",
    "        \"device\": device,\n",
    "        \"num_layers\": 3,\n",
    "        \"ensemble_size\": ensemble_size,\n",
    "        \"hid_size\": 200,\n",
    "        \"in_size\": \"???\",\n",
    "        \"out_size\": \"???\",\n",
    "        \"deterministic\": False,\n",
    "        \"propagation_method\": \"fixed_model\",\n",
    "        # can also configure activation function for GaussianMLP\n",
    "        \"activation_fn_cfg\": {\n",
    "            \"_target_\": \"torch.nn.LeakyReLU\",\n",
    "            \"negative_slope\": 0.01\n",
    "        }\n",
    "    },\n",
    "    # options for training the dynamics model\n",
    "    \"algorithm\": {\n",
    "        \"learned_rewards\": False,\n",
    "        \"target_is_delta\": True,\n",
    "        \"normalize\": True,\n",
    "    },\n",
    "    # these are experiment specific options\n",
    "    \"overrides\": {\n",
    "        \"trial_length\": trial_length,\n",
    "        \"num_steps\": num_trials * trial_length,\n",
    "        \"model_batch_size\": 256,\n",
    "        \"validation_ratio\": 0.05\n",
    "    }\n",
    "}\n",
    "\n",
    "cfg = OmegaConf.create(cfg_dict)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "dynamics_model = common_utils.create_one_dim_tr_model(cfg, obs_shape, act_shape)\n",
    "model_env = models.ModelEnv(env, dynamics_model, term_fn, reward_fn, generator=generator)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "replay_buffer = common_utils.create_replay_buffer(cfg, obs_shape, act_shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[np.float64(-108.446508305019),\n",
       " np.float64(250.85743183220694),\n",
       " np.float64(-74.10173335936713)]"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "common_utils.rollout_agent_trajectories(\n",
    "    env=env,\n",
    "    steps_or_trials_to_collect=trial_length,\n",
    "    agent=planning.RandomAgent(env),\n",
    "    agent_kwargs={},\n",
    "    replay_buffer=replay_buffer,\n",
    "    trial_length=trial_length,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "agent_cfg = OmegaConf.create({\n",
    "    # this class evaluates many trajectories and picks the best one\n",
    "    \"_target_\": \"deeptrade.planning.TrajectoryOptimizerAgent\",\n",
    "    \"planning_horizon\": 15,\n",
    "    \"replan_freq\": 1,\n",
    "    \"verbose\": False,\n",
    "    \"action_lb\": \"???\",\n",
    "    \"action_ub\": \"???\",\n",
    "    # this is the optimizer to generate and choose a trajectory\n",
    "    \"optimizer_cfg\": {\n",
    "        \"_target_\": \"deeptrade.planning.CEMOptimizer\",\n",
    "        \"num_iterations\": 5,\n",
    "        \"elite_ratio\": 0.1,\n",
    "        \"population_size\": 500,\n",
    "        \"alpha\": 0.1,\n",
    "        \"device\": device,\n",
    "        \"lower_bound\": \"???\",\n",
    "        \"upper_bound\": \"???\",\n",
    "        \"return_mean_elites\": True,\n",
    "        \"clipped_normal\": False\n",
    "    }\n",
    "})\n",
    "\n",
    "agent = planning.create_trajectory_optim_agent_for_model(\n",
    "    model_env,\n",
    "    agent_cfg,\n",
    "    num_particles=20\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [],
   "source": [
    "model_trainer = models.ModelTrainer(dynamics_model, optim_lr=1e-3, weight_decay=5e-5)\n",
    "# dynamics_model.update_normalizer(replay_buffer.get_all())  # update normalizer stats\n",
    "all_rewards = []\n",
    "\n",
    "for trial in range(num_trials):\n",
    "    obs, _ = env.reset()\n",
    "    agent.reset()\n",
    "    \n",
    "    terminated = False\n",
    "    total_reward = 0.0\n",
    "    steps_trial = 0\n",
    "    while not terminated:\n",
    "        if steps_trial == 0:\n",
    "            dynamics_model.update_normalizer(replay_buffer.get_all()) # update normalizer stats \n",
    "            \n",
    "            dataset_train, dataset_val = common_utils.get_basic_buffer_iterators(\n",
    "                replay_buffer,\n",
    "                batch_size=cfg.overrides.model_batch_size,\n",
    "                val_ratio=cfg.overrides.validation_ratio,\n",
    "                ensemble_size=ensemble_size,\n",
    "                shuffle_each_epoch=True,\n",
    "                bootstrap_permutes=False,  # build bootstrap dataset using sampling with replacement\n",
    "            )\n",
    "            \n",
    "            model_trainer.train(\n",
    "                dataset_train, \n",
    "                dataset_val=dataset_val, \n",
    "                num_epochs=50,\n",
    "                silent=True\n",
    "            )\n",
    "        # print(obs, env.unwrapped.time)\n",
    "        next_obs, reward, terminated, truncated, _ = common_utils.step_env_and_add_to_buffer(\n",
    "            env=env,\n",
    "            obs=obs,\n",
    "            agent=agent,\n",
    "            agent_kwargs={},\n",
    "            replay_buffer=replay_buffer\n",
    "        )\n",
    "        obs = next_obs\n",
    "        total_reward += reward\n",
    "        steps_trial += 1\n",
    "        \n",
    "        if steps_trial == trial_length or truncated:\n",
    "            break\n",
    "    \n",
    "    all_rewards.append(total_reward)\n",
    "\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "all_rewards\n",
    "fig, ax = plt.subplots(1, 1, figsize=(10, 5))\n",
    "ax.plot(all_rewards)\n",
    "ax.set_title(\"Rewards\")\n",
    "fig.show()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "deeptrade-env",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.15"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
